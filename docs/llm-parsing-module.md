# ğŸ§  LLM Parsing Module

## Overview

The LLM Parsing Module enables client-side extraction of structured data from unstructured user input using any Large Language Model (LLM) provider. It is designed to be reusable, lightweight, and cost-efficient, supporting use cases such as transforming freeform text into structured fields for profile building.

This module powers features like parsing pasted federal experience descriptions into structured job fields (`title`, `agency`, `description`, etc.) in the FedEReady application.

---

## âœ¨ Features

- âœ… **Provider-Agnostic**: Compatible with OpenAI, Mistral, AWS Bedrock, etc.
- âœ… **Instance-Based Routing**: Supports different agents/models per task
- âœ… **Prompt Management**: Prompts defined locally or by external LLM services
- âœ… **Flexible Output Modes**: Supports `raw` text or validated `json` parsing
- âœ… **Client-Side Rate Limiting**: Usage caps per client to control costs
- âœ… **Modular and Extendable**: Each component can evolve independently

---

## ğŸ”§ Core Use Case

**Input**: A freeform pasted job description from USAJobs  
**Output**: A validated JSON object like:

```json
{
	"title": "Program Analyst",
	"agency": "Department of Veterans Affairs",
	"description": "Responsible for coordinating and analyzing program operations...",
	"level": "GS-11",
	"start_date": "2019-06",
	"end_date": "2023-01"
}
```

---

## ğŸ—ï¸ Architecture

```txt
[React Frontend App]
  |
  |-- [ParserModule.ts]
         |
         â”œâ”€â”€ ProviderAdapterFactory
         â”‚     â”œâ”€â”€ OpenAIAdapter
         â”‚     â”œâ”€â”€ MistralAdapter
         â”‚     â””â”€â”€ BedrockAdapter
         |
         â”œâ”€â”€ PromptManager
         â”‚     â””â”€â”€ Loads templates or references external agents
         |
         â”œâ”€â”€ OutputParser
         â”‚     â”œâ”€â”€ RawParser
         â”‚     â””â”€â”€ JSONValidator
         |
         â””â”€â”€ RateLimiter
               â””â”€â”€ Tracks usage per client (via localStorage)
```

---

## ğŸ“¦ Module Interface

```ts
type Provider = 'openai' | 'mistral' | 'bedrock'

interface ParseOptions {
	provider: Provider
	instanceId: string
	promptId: string
	input: string
	mode?: 'json' | 'raw'
	maxTokens?: number
}

async function parseWithLLM(opts: ParseOptions): Promise<ParsedOutput>
```

---

## ğŸ§  Prompt Format Example

```txt
Extract the following fields as JSON:
- title
- agency
- description
- level (optional)
- start_date (optional)
- end_date (optional)

If a field is missing, return null.
Text: {{input}}
```

Prompts can be managed:

- Locally (e.g., Markdown or TS templates)
- On the LLM service (e.g., OpenAI Assistants or Bedrock Agents)

---

## ğŸ”’ Client Usage Control

To control cost and avoid abuse:

- Parsing is limited to N calls per month
- Usage is tracked in `localStorage`
- Future plans include syncing usage with Supabase if the user is logged in

---

## ğŸ”® Extensibility Ideas

| Feature            | Description                                   |
| ------------------ | --------------------------------------------- |
| Multi-turn support | Enable follow-up parsing workflows            |
| Prompt versioning  | Avoid breaking changes with stored prompts    |
| Streaming support  | Live updates from LLMs that support streaming |
| Fallback providers | Retry with secondary LLM if the first fails   |

---

## ğŸš€ Why It Matters

This module demonstrates thoughtful modular architecture, cost-aware client design, and scalable prompt-driven parsingâ€”all key skills in modern AI-enhanced app development.
